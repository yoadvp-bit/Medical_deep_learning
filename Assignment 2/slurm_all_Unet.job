#!/bin/bash

#SBATCH --partition=gpu_mig
#SBATCH --gpus=1
#SBATCH --job-name=HyperparamSweep
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=18:00:00  # Increased time limit for more runs
#SBATCH --output=slurm_output_%A.out

module purge
module load 2023
module load PyTorch/2.1.2-foss-2023a-CUDA-12.1.1

cd $HOME/AI_for_medical_imaging_course/Assignment\ 2
source /gpfs/work5/0/prjs1312/venv2/bin/activate

# Define hyperparameter values
learning_rates=(0.0001, 0.001, 0.01)
batch_sizes=(32, 64)
epochs=(15)
channels=("16,32,64,128", "32,64,128,256", "16,32,64,128,256,512")
optimizers=("adam")

# Run all hyperparameter combinations sequentially
for lr in "${learning_rates[@]}"; do
    for batch_size in "${batch_sizes[@]}"; do
        for epoch in "${epochs[@]}"; do
            for channels in "${channels[@]}"; do
                for optimizer in "${optimizers[@]}"; do
                    echo "Running: lr=$lr, batch_size=$batch_size, epochs=$epoch, channels=$channels,  optimizer=$optimizer"
                    srun python -u main_UNET.py --checkpoint_folder_save checkpoints/${optimizer_name}_lr${optimizer_lr}_bs${batch_size}_ch${channels}/
                                            --optimizer_lr $lr \
                                            --batch_size $batch_size \
                                            --max_epochs $epoch \
                                            --channels $channels \
                                            --optimizer_name $optimizer \
                                            --experiment_name ${optimizer_name}_lr${optimizer_lr}_bs${batch_size}_ch${channels}
                    sleep 5  # Small delay to avoid overwhelming the scheduler
                done
            done
        done
    done
done